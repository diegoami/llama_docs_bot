{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Bottoms-Up Development - Evaluation Baseline\n",
    "\n",
    "LlamaIndex provides some basic evaluation of query engines! We can setup an evaluator that will measure both hallucinations, as well as if the query was actually answered!\n",
    "\n",
    "This is provided by two main evaluations:\n",
    "\n",
    "- `ResponseSourceEvaluator` - uses an LLM to decide if the response is similar enough to the sources -- a good measure for hallunication detection!\n",
    "- `QueryResponseEvaluator` - uses an LLM to decide if a response is similar enough to the original query -- a good measure for checking if the query was answered!\n",
    "\n",
    "You may have noticed that we are using an LLM for this task. That means we will want to pick a powerful LLM, like GPT-4 or Claude-2.\n",
    "\n",
    "Lastly, using these methods, we can also use the LLM to generate syntheic questions to evaluate with!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Baseline Query Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\"\n",
    "#openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_docs_bot.markdown_docs_reader import MarkdownDocsReader\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "def load_markdown_docs(filepath):\n",
    "    \"\"\"Load markdown docs from a directory, excluding all other file types.\"\"\"\n",
    "    loader = SimpleDirectoryReader(\n",
    "        input_dir=filepath, \n",
    "        required_exts=[\".md\"],\n",
    "        file_extractor={\".md\": MarkdownDocsReader()},\n",
    "        recursive=True\n",
    "    )\n",
    "\n",
    "    documents = loader.load_data()\n",
    "\n",
    "    # exclude some metadata from the LLM\n",
    "    for doc in documents:\n",
    "        doc.excluded_llm_metadata_keys = [\"File Name\", \"Content Type\", \"Header Path\"]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our documents from each folder.\n",
    "# we keep them seperate for now, in order to create seperate indexes later\n",
    "getting_started_docs = load_markdown_docs(\"../docs/getting_started\")\n",
    "community_docs = load_markdown_docs(\"../docs/community\")\n",
    "data_docs = load_markdown_docs(\"../docs/core_modules/data_modules\")\n",
    "agent_docs = load_markdown_docs(\"../docs/core_modules/agent_modules\")\n",
    "model_docs = load_markdown_docs(\"../docs/core_modules/model_modules\")\n",
    "query_docs = load_markdown_docs(\"../docs/core_modules/query_modules\")\n",
    "supporting_docs = load_markdown_docs(\"../docs/core_modules/supporting_modules\")\n",
    "tutorials_docs = load_markdown_docs(\"../docs/end_to_end_tutorials\")\n",
    "contributing_docs = load_markdown_docs(\"../docs/development\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, set_global_service_context\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# create a global service context\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0))\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "\n",
    "# create a vector store index for each folder\n",
    "try:\n",
    "    getting_started_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./getting_started_index\"))\n",
    "    community_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./community_index\"))\n",
    "    data_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./data_index\"))\n",
    "    agent_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./agent_index\"))\n",
    "    model_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./model_index\"))\n",
    "    query_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./query_index\"))\n",
    "    supporting_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./supporting_index\"))\n",
    "    tutorials_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./tutorials_index\"))\n",
    "    contributing_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./contributing_index\"))\n",
    "except:\n",
    "    getting_started_index = VectorStoreIndex.from_documents(getting_started_docs)\n",
    "    getting_started_index.storage_context.persist(persist_dir=\"./getting_started_index\")\n",
    "\n",
    "    community_index = VectorStoreIndex.from_documents(community_docs)\n",
    "    community_index.storage_context.persist(persist_dir=\"./community_index\")\n",
    "\n",
    "    data_index = VectorStoreIndex.from_documents(data_docs)\n",
    "    data_index.storage_context.persist(persist_dir=\"./data_index\")\n",
    "\n",
    "    agent_index = VectorStoreIndex.from_documents(agent_docs)\n",
    "    agent_index.storage_context.persist(persist_dir=\"./agent_index\")\n",
    "\n",
    "    model_index = VectorStoreIndex.from_documents(model_docs)\n",
    "    model_index.storage_context.persist(persist_dir=\"./model_index\")\n",
    "\n",
    "    query_index = VectorStoreIndex.from_documents(query_docs)\n",
    "    query_index.storage_context.persist(persist_dir=\"./query_index\")    \n",
    "\n",
    "    supporting_index = VectorStoreIndex.from_documents(supporting_docs)\n",
    "    supporting_index.storage_context.persist(persist_dir=\"./supporting_index\")\n",
    "\n",
    "    tutorials_index = VectorStoreIndex.from_documents(tutorials_docs)\n",
    "    tutorials_index.storage_context.persist(persist_dir=\"./tutorials_index\")\n",
    "\n",
    "    contributing_index = VectorStoreIndex.from_documents(contributing_docs)\n",
    "    contributing_index.storage_context.persist(persist_dir=\"./contributing_index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Query Engine Tools\n",
    "\n",
    "Since we have so many indicies, we can create a query engine tool for each and then use them in a single query engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool\n",
    "\n",
    "# create a query engine tool for each folder\n",
    "getting_started_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=getting_started_index.as_query_engine(), \n",
    "    name=\"Getting Started\", \n",
    "    description=\"Useful for answering questions about installing and running llama index, as well as basic explanations of how llama index works.\"\n",
    ")\n",
    "\n",
    "community_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=community_index.as_query_engine(),\n",
    "    name=\"Community\",\n",
    "    description=\"Useful for answering questions about integrations and other apps built by the community.\"\n",
    ")\n",
    "\n",
    "data_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=data_index.as_query_engine(),\n",
    "    name=\"Data Modules\",\n",
    "    description=\"Useful for answering questions about data loaders, documents, nodes, and index structures.\"\n",
    ")\n",
    "\n",
    "agent_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=agent_index.as_query_engine(),\n",
    "    name=\"Agent Modules\",\n",
    "    description=\"Useful for answering questions about data agents, agent configurations, and tools.\"\n",
    ")\n",
    "\n",
    "model_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=model_index.as_query_engine(),\n",
    "    name=\"Model Modules\",\n",
    "    description=\"Useful for answering questions about using and configuring LLMs, embedding modles, and prompts.\"\n",
    ")\n",
    "\n",
    "query_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_index.as_query_engine(),\n",
    "    name=\"Query Modules\",\n",
    "    description=\"Useful for answering questions about query engines, query configurations, and using various parts of the query engine pipeline.\"\n",
    ")\n",
    "\n",
    "supporting_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=supporting_index.as_query_engine(),\n",
    "    name=\"Supporting Modules\",\n",
    "    description=\"Useful for answering questions about supporting modules, such as callbacks, service context, and avaluation.\"\n",
    ")\n",
    "\n",
    "tutorials_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=tutorials_index.as_query_engine(),\n",
    "    name=\"Tutorials\",\n",
    "    description=\"Useful for answering questions about end-to-end tutorials and giving examples of specific use-cases.\"\n",
    ")\n",
    "\n",
    "contributing_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=contributing_index.as_query_engine(),\n",
    "    name=\"Contributing\",\n",
    "    description=\"Useful for answering questions about contributing to llama index, including how to contribute to the codebase and how to build documentation.\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Unified Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[\n",
    "        getting_started_tool,\n",
    "        community_tool,\n",
    "        data_tool,\n",
    "        agent_tool,\n",
    "        model_tool,\n",
    "        query_tool,\n",
    "        supporting_tool,\n",
    "        tutorials_tool,\n",
    "        contributing_tool\n",
    "    ],\n",
    "    # enable this for streaming\n",
    "    # response_synthesizer=get_response_synthesizer(streaming=True),\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Query Engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install Llama Index, you can follow these steps:\n",
      "\n",
      "1. Clone the repository by running the following command in your terminal:\n",
      "   `git clone https://github.com/jerryjliu/llama_index.git`\n",
      "\n",
      "2. Once the repository is cloned, navigate to the cloned directory.\n",
      "\n",
      "3. If you want to do an editable install (where you can modify source files), run the command:\n",
      "   `pip install -e .`\n",
      "\n",
      "4. If you want to install optional dependencies and dependencies used for development (such as unit testing), run the command:\n",
      "   `pip install -r requirements.txt`\n",
      "\n",
      "After completing these steps, Llama Index should be downloaded and installed on your system.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I install llama index?\")\n",
    "print(str(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Basline!\n",
    "\n",
    "Now that we have our baseline query engine created, we can create a basic evaluation pipeline!\n",
    "\n",
    "Our pipeline will:\n",
    "\n",
    "- Generate a small dataset of questions\n",
    "- Save/cache these questions (so we can properly compare performance later!)\n",
    "- Evaluate both response quality and hallucination\n",
    "\n",
    "To do this reliably, we need to use an LLM smarter than `gpt-3.5-turbo`, so we will setup `gpt-4` for the evaluation process!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Dataset\n",
    "\n",
    "In order to make the question generation more effecient, we can remove small documents and combine all documents into a giant single docoument.\n",
    "\n",
    "I also modify the question generation prompt, to generate a single question for each chunk, along with extra context for what it is reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../docs\", recursive=True, required_exts=[\".md\"]).load_data()\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for doc in documents:\n",
    "    all_text += doc.text\n",
    "\n",
    "giant_document = Document(text=all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.prompts import Prompt\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "\n",
    "gpt4_service_context = ServiceContext.from_defaults(llm=OpenAI(llm=\"gpt-4\", temperature=0))\n",
    "\n",
    "question_dataset = []\n",
    "if os.path.exists(\"question_dataset.txt\"):\n",
    "    with open(\"question_dataset.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            question_dataset.append(line.strip())\n",
    "else:\n",
    "    # generate questions\n",
    "    data_generator = DatasetGenerator.from_documents(\n",
    "        [giant_document],\n",
    "        text_question_template=Prompt(\n",
    "            \"A sample from the LlamaIndex documentation is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Using the documentation sample, carefully follow the instructions below:\\n\"\n",
    "            \"{query_str}\"\n",
    "        ),\n",
    "        question_gen_query=(\n",
    "            \"You are an evaluator for a search pipeline. Your task is to write a single question \"\n",
    "            \"using the provided documentation sample above to test the search pipeline. The question should \"\n",
    "            \"reference specific names, functions, and terms. Restrict the question to the \"\n",
    "            \"context information provided.\\n\"\n",
    "            \"Question: \"\n",
    "        ),\n",
    "        # set this to be low, so we can generate more questions\n",
    "        service_context=gpt4_service_context\n",
    "    )\n",
    "    generated_questions = data_generator.generate_questions_from_nodes()\n",
    "\n",
    "    # randomly pick 40 questions from each dataset\n",
    "    generated_questions = random.sample(generated_questions, 40)\n",
    "    question_dataset.extend(generated_questions)\n",
    "\n",
    "    print(f\"Generated {len(question_dataset)} questions.\")\n",
    "\n",
    "    # save the questions!\n",
    "    with open(\"question_dataset.txt\", \"w\") as f:\n",
    "        for question in question_dataset:\n",
    "            f.write(f\"{question.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are the node postprocessors available in the LlamaIndex documentation?', 'What are the available options for the storage backend of the index store in LlamaIndex?', 'What are the three primary sections within the layout of the ChatView component?', 'What embedding model does LlamaIndex use by default?', 'What is the purpose of the `load_collection_model` function in the LlamaIndex documentation?']\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(question_dataset, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with the Dataset\n",
    "\n",
    "Now that we have our dataset, let's measure performance!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Response for Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index import Response\n",
    "\n",
    "def evaluate_query_engine(evaluator, query_engine, questions):\n",
    "    async def run_query(query_engine, q):\n",
    "        try:\n",
    "            return await query_engine.aquery(q)\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for batch_size in range(0, len(questions), 5):\n",
    "        batch_qs = questions[batch_size:batch_size+5]\n",
    "\n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
    "\n",
    "        for response in responses:\n",
    "            if evaluator.evaluate_response(response=response).passing: \n",
    "                eval_result = 1\n",
    "            else:\n",
    "                eval_result = 0\n",
    "            total_correct += eval_result\n",
    "            all_results.append(eval_result)\n",
    "\n",
    "        \n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished batch 1 out of 8\n",
      "finished batch 2 out of 8\n",
      "finished batch 3 out of 8\n",
      "finished batch 4 out of 8\n",
      "finished batch 5 out of 8\n",
      "finished batch 6 out of 8\n",
      "finished batch 7 out of 8\n",
      "finished batch 8 out of 8\n",
      "Hallucination? Scored 31 out of 40 questions correctly.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "# gpt-4 evaluator!\n",
    "evaluator = FaithfulnessEvaluator(service_context=gpt4_service_context)\n",
    "\n",
    "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
    "\n",
    "print(f\"Hallucination? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How can I convert tools to LangChain tools using the provided documentation sample?'\n",
      " 'What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?'\n",
      " 'What are the available options for the storage backend of the index store in LlamaIndex?'\n",
      " 'What is the purpose of the \"router query engine\" in the LlamaIndex framework?'\n",
      " 'What is the purpose of the `fetchDocuments` function in the `fetchDocuments.tsx` file in the React frontend?'\n",
      " \"What is the function used to retrieve the collections for the logged-in user in the Delphic project's frontend?\"\n",
      " 'What is the purpose of the Algovera tool built on top of LlamaIndex?'\n",
      " 'What are the three primary sections within the layout of the ChatView component?'\n",
      " 'What is the purpose of the SQLTableNodeMapping object in the LlamaIndex documentation sample?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hallucinated_questions = np.array(question_dataset)[np.array(all_results) == 0]\n",
    "print(hallucinated_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation is not provided in the given context information.\n",
      "-----------------\n",
      "> Source (Doc id: 2a787a34-1b5d-4b64-860d-6c945c58ffa0): Sub question: What is the purpose of the GuidancePydanticProgram class?\n",
      "Response: The purpose of the GuidancePydanticProgram class is not provided in the given context information.\n",
      "\n",
      "> Source (Doc id: 7af36a27-d9a2-4230-9491-9ef44b27ebd9): Sub question: How do I use and configure LLMs?\n",
      "Response: To use and configure LLMs, you can refer to the code snippet provided in the context information. Additionally, if you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `ServiceContext` since the information is not available by default.\n",
      "\n",
      "> Source (Doc id: cc4520d5-38bc-472a-bac7-386818b6d063): Sub question: What is the purpose of the Query Modules?\n",
      "Response: The purpose of the Query Modules is to provide a generic interface, called the query engine, that allows users to ask questions over their data. The query engine takes in natural language queries and returns rich responses. It is often built on one or many Indices via Retrievers, and users can compose multiple query engines to achieve more advanced capabilities. Additionally, the Query Modules include query transformations, which are modules that convert queries into other queries. These transformations can be single-step or multi-step, depending on the desired functionality.\n",
      "\n",
      "> Source (Doc id: 04c92cd7-f662-48a0-bafe-03f37b9bf347): Sub question: How do I use the query engine pipeline?\n",
      "Response: To use the query engine pipeline, you need to create a query engine object and then use the query method to pass in your natural language query. The query engine will then return a response based on the query. In the provided code snippet, the query engine is created using the `index.as_query_engine()` method with the `streaming=True` parameter. The `query()` method is then called on the query engine object with the query \"What did the author do growing up?\". Finally, the `print_response_stream()` method is used to print the response stream.\n",
      "\n",
      "> Source (Doc id: f5690c9b-d941-4062-8aa8-fb631a9e9e1f): Sub question: What is the purpose of the Supporting Modules?\n",
      "Response: The purpose of the Supporting Modules is to provide additional functionality and support for the core modules in the system. These modules are designed to enhance the capabilities of the core modules and assist in various tasks and processes.\n",
      "\n",
      "> Source (Doc id: e6aedd39-7ac6-4ade-be49-7add0df533a2): Sub question: How do I use callbacks in llama index?\n",
      "Response: To use callbacks in LlamaIndex, you can implement your own callback or use an existing one. Callbacks in LlamaIndex are used to help debug, track, and trace the inner workings of the library. The callback manager allows you to add as many callbacks as needed.\n",
      "\n",
      "Callbacks in LlamaIndex can be used to log data related to events, track the duration and number of occurrences of each event, and record a trace map of events. For example, the `LlamaDebugHandler` callback will print the trace of events after most operations by default.\n",
      "\n",
      "There are several event types available to be tracked with callbacks, such as CHUNKING, NODE_PARSING, EMBEDDING, LLM, QUERY, RETRIEVE, SYNTHESIZE, TREE, and SUB_QUESTIONS. Each callback may not leverage every event type, but you can choose which events to track based on your needs.\n",
      "\n",
      "By using callbacks in LlamaIndex, you can customize the tracking and tracing of events to suit your requirements and...\n",
      "\n",
      "> Source (Doc id: 36daf570-279f-4d78-8e43-33322e1937dd): Sub question: What is the purpose of the Tutorials?\n",
      "Response: The purpose of the tutorials is to provide information and guidance on various topics, including using different indexes for different use cases, storing global state values, customizing internal prompts, and reading text from images.\n",
      "\n",
      "> Source (Doc id: 943ebbb4-d006-455a-bcb5-5c2c28205021): Sub question: Are there any end-to-end tutorials available?\n",
      "Response: Yes, there are end-to-end tutorials available.\n",
      "\n",
      "> Source (Doc id: f38530bf-7548-4314-abef-419ddf120812): Sub question: What is the purpose of the Contributing section?\n",
      "Response: The purpose of the Contributing section is to provide guidelines and instructions for individuals who want to contribute to the development of LLamaIndex. It likely includes information on how to submit bug reports, suggest new features, and contribute code or documentation to the project.\n",
      "\n",
      "> Source (Doc id: 01c8955d-657f-4a55-b045-b0f8521542f6): Sub question: How can I contribute to the codebase of llama index?\n",
      "Response: To contribute to the codebase of LLamaIndex, you can follow the guidelines provided by the project. Typically, open-source projects have a set of instructions on how to contribute, including steps for setting up the development environment, guidelines for coding style and documentation, and a process for submitting pull requests. You can check the LLamaIndex documentation or reach out to the project maintainers for specific instructions on how to contribute to their codebase.\n",
      "\n",
      "> Source (Doc id: b679a427-ac2c-4dfa-8ec8-be7c1dd48504): from llama_index import Prompt\n",
      "\n",
      "template = (\n",
      "    \"We have provided context information below. \\n\"\n",
      "    \"---------------------\\n\"\n",
      "    \"{context_str}\"\n",
      "    \"\\n---------------------\\n\"\n",
      "    \"Given this information, please answer the question: {query_str}\\n\"\n",
      ")\n",
      "qa_template = Prompt(template)\n",
      "\n",
      "> Source (Doc id: 9b0b90b3-8775-4e77-805e-5e415414aead): retriever = index.as_retriever()\n",
      "synth = get_response_synthesizer(\n",
      "    text_qa_template=<custom_qa_prompt>,\n",
      "    refine_template=<custom_refine_prompt>\n",
      ")\n",
      "query_engine = RetrieverQueryEngine(retriever, response_synthesizer)\n",
      "\n",
      "> Source (Doc id: 5efb60b1-a340-4872-a8a1-4299d8401dfe): The following code snippet shows how you can get started using LLMs.\n",
      "\n",
      "> Source (Doc id: 8822a0cf-ffcf-412d-be08-1265c57281d2): If you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `ServiceContext` since the information is not available by default.\n",
      "\n",
      "> Source (Doc id: 5eb95361-c932-42eb-bc41-47acd3f4b1ea): Query engine is a generic interface that allows you to ask question over your data.\n",
      "\n",
      "A query engine takes in a natural language query, and returns a rich response.\n",
      "It is most often (but not always) built on one or many Indices via Retrievers.\n",
      "You can compose multiple query engines to achieve more advanced capability.\n",
      "\n",
      "> Source (Doc id: 46fafbef-4709-48a0-93bc-29226603b4c8): LlamaIndex allows you to perform *query transformations* over your index structures.\n",
      "Query transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index. \n",
      "\n",
      "They can also be **multi-step**, as in: \n",
      "1. The query is transformed, executed against an index, \n",
      "2. The response is retrieved.\n",
      "3. Subsequent queries are transformed/executed in a sequential fashion.\n",
      "\n",
      "We list some of our query transformations in more detail below.\n",
      "\n",
      "> Source (Doc id: 5eb95361-c932-42eb-bc41-47acd3f4b1ea): Query engine is a generic interface that allows you to ask question over your data.\n",
      "\n",
      "A query engine takes in a natural language query, and returns a rich response.\n",
      "It is most often (but not always) built on one or many Indices via Retrievers.\n",
      "You can compose multiple query engines to achieve more advanced capability.\n",
      "\n",
      "> Source (Doc id: 5ac1b5d2-bf24-455d-be4d-e10dba925999): query_engine = index.as_query_engine(\n",
      "    streaming=True,\n",
      ")\n",
      "streaming_response = query_engine.query(\n",
      "    \"What did the author do growing up?\", \n",
      ")\n",
      "streaming_response.print_response_stream()\n",
      "\n",
      "> Source (Doc id: 8b3d7498-2792-4fb4-86e8-6dfd6d3823fb): Notebooks with usage of these components can be found below.\n",
      "\n",
      "> Source (Doc id: 890f0a3f-10ea-469a-bf20-1b2cad8f3c07): Notebooks with usage of these components can be found below.\n",
      "\n",
      "> Source (Doc id: c3f855af-10c8-44b0-9259-f91ad8431926): LlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library. \n",
      "Using the callback manager, as many callbacks as needed can be added.\n",
      "\n",
      "In addition to logging data related to events, you can also track the duration and number of occurances\n",
      "of each event. \n",
      "\n",
      "Furthermore, a trace map of events is also recorded, and callbacks can use this data\n",
      "however they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events\n",
      "after most operations.\n",
      "\n",
      "**Callback Event Types**  \n",
      "While each callback may not leverage each event type, the following events are available to be tracked:\n",
      "\n",
      "- `CHUNKING` -> Logs for the before and after of text splitting.\n",
      "- `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into.\n",
      "- `EMBEDDING` -> Logs for the number of texts embedded.\n",
      "- `LLM` -> Logs for the template and response of LLM calls.\n",
      "- `QUERY` -> Keeps track of the start and end of each query.\n",
      "- `RETRIEVE` -> Logs for the nodes retri...\n",
      "\n",
      "> Source (Doc id: 4146efe5-ec35-47f8-9a33-aed53373bba1): In addition to evaluating queries, LlamaIndex can also use your data to generate questions to evaluate on. This means that you can automatically generate questions, and then run an evaluation pipeline to test if the LLM can actually answer questions accurately using your data.\n",
      "\n",
      "> Source (Doc id: 78549719-70c1-45d7-9476-882b7be11ce9): With our base app working, it might feel like a lot of work to build up a useful index. What if we gave the user some kind of starting point to show off the app's query capabilities? We can do just that! First, let's make a small change to our app so that we save the index to disk after every upload:\n",
      "\n",
      "> Source (Doc id: 170295ff-8c25-45dc-bd52-1a4725cd2590): In this tutorial, we covered a ton of information, while solving some common issues and problems along the way:\n",
      "\n",
      "- Using different indexes for different use cases (List vs. Vector index)\n",
      "- Storing global state values with Streamlit's `session_state` concept\n",
      "- Customizing internal prompts with Llama Index\n",
      "- Reading text from images with Llama Index\n",
      "\n",
      "The final version of this tutorial can be found here and a live hosted demo is available on Huggingface Spaces.\n",
      "\n",
      "> Source (Doc id: 57eb560f-e25f-4bfa-83be-23b6aa50bfba): To deploy the app, you're going to need Docker and Docker Compose installed. If you're on Ubuntu or another, common\n",
      "Linux distribution, DigitalOcean has\n",
      "a great Docker tutorial and\n",
      "another great tutorial\n",
      "for Docker Compose\n",
      "you can follow. If those don't work for you, try\n",
      "the official docker documentation.\n",
      "\n",
      "> Source (Doc id: df3d35c8-66f7-4d02-9680-2676b28de531): LlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit.\n",
      "\n",
      "We provide tutorials and resources to help you get started in this area.\n",
      "\n",
      "Relevant Resources:\n",
      "- Fullstack Application Guide\n",
      "- Fullstack Application with Delphic\n",
      "- A Guide to Extracting Terms and Definitions\n",
      "- LlamaIndex Starter Pack\n",
      "\n",
      "> Source (Doc id: 2973f912-b247-4da3-9c0b-c751d30a0c39): By default, LLamaIndex sends your data to OpenAI for generating embeddings and natural language responses. However, it is important to note that this can be configured according to your preferences. LLamaIndex provides the flexibility to use your own embedding model or run a large language model locally if desired.\n",
      "\n",
      "> Source (Doc id: f671763a-cec8-419e-920d-3f066a9ac3ea): Regarding data privacy, when using LLamaIndex with OpenAI, the privacy details and handling of your data are subject to OpenAI's policies. And each custom service other than OpenAI have their own policies as well.\n",
      "\n",
      "> Source (Doc id: 2973f912-b247-4da3-9c0b-c751d30a0c39): By default, LLamaIndex sends your data to OpenAI for generating embeddings and natural language responses. However, it is important to note that this can be configured according to your preferences. LLamaIndex provides the flexibility to use your own embedding model or run a large language model locally if desired.\n",
      "\n",
      "> Source (Doc id: 423dc347-cc31-4ac0-ae83-6308085a8b2d): LLamaIndex offers modules to connect with other vector stores within indexes to store embeddings. It is worth noting that each vector store has its own privacy policies and practices, and LLamaIndex does not assume responsibility for how they handle or use your data. Also by default LLamaIndex have a default option to store your embeddings locally.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?')\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `RouterQueryEngine` in LlamaIndex is to allow for query transformations over index structures. It can be used in the search pipeline by taking in a natural language query and returning a rich response. The `RouterQueryEngine` is often built on one or many Indices via Retrievers, and multiple query engines can be composed together to achieve more advanced capability.\n",
      "-----------------\n",
      "> Source (Doc id: 30a5b893-072d-4344-ae6e-af10a27d85f4): Sub question: What is the purpose of the `RouterQueryEngine` in LlamaIndex?\n",
      "Response: The purpose of the `RouterQueryEngine` in LlamaIndex is to allow you to perform query transformations over your index structures. Query transformations are modules that convert a query into another query. They can be single-step, where the transformation is run once before the query is executed against an index, or multi-step, where the query is transformed, executed against an index, the response is retrieved, and subsequent queries are transformed and executed in a sequential fashion. The `RouterQueryEngine` is a generic interface that allows you to ask questions over your data by taking in a natural language query and returning a rich response. It is often built on one or many Indices via Retrievers, and you can compose multiple query engines to achieve more advanced capability.\n",
      "\n",
      "> Source (Doc id: 3744ee1b-e002-4110-ae11-69779d821626): Sub question: How can the `RouterQueryEngine` be used in the search pipeline?\n",
      "Response: The `RouterQueryEngine` can be used in the search pipeline by taking in a natural language query and returning a rich response. It is often built on one or many Indices via Retrievers. Multiple query engines can be composed together to achieve more advanced capability.\n",
      "\n",
      "> Source (Doc id: 46fafbef-4709-48a0-93bc-29226603b4c8): LlamaIndex allows you to perform *query transformations* over your index structures.\n",
      "Query transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index. \n",
      "\n",
      "They can also be **multi-step**, as in: \n",
      "1. The query is transformed, executed against an index, \n",
      "2. The response is retrieved.\n",
      "3. Subsequent queries are transformed/executed in a sequential fashion.\n",
      "\n",
      "We list some of our query transformations in more detail below.\n",
      "\n",
      "> Source (Doc id: 5eb95361-c932-42eb-bc41-47acd3f4b1ea): Query engine is a generic interface that allows you to ask question over your data.\n",
      "\n",
      "A query engine takes in a natural language query, and returns a rich response.\n",
      "It is most often (but not always) built on one or many Indices via Retrievers.\n",
      "You can compose multiple query engines to achieve more advanced capability.\n",
      "\n",
      "> Source (Doc id: 5eb95361-c932-42eb-bc41-47acd3f4b1ea): Query engine is a generic interface that allows you to ask question over your data.\n",
      "\n",
      "A query engine takes in a natural language query, and returns a rich response.\n",
      "It is most often (but not always) built on one or many Indices via Retrievers.\n",
      "You can compose multiple query engines to achieve more advanced capability.\n",
      "\n",
      "> Source (Doc id: 5ac1b5d2-bf24-455d-be4d-e10dba925999): query_engine = index.as_query_engine(\n",
      "    streaming=True,\n",
      ")\n",
      "streaming_response = query_engine.query(\n",
      "    \"What did the author do growing up?\", \n",
      ")\n",
      "streaming_response.print_response_stream()\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the purpose of the `RouterQueryEngine` in LlamaIndex and how can it be used in the search pipeline?')\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=1000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Response for Answer Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from llama_index import Response\n",
    "\n",
    "def evaluate_query_engine(evaluator, query_engine, questions):\n",
    "    async def run_query(query_engine, q):\n",
    "        try:\n",
    "            return await query_engine.aquery(q)\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for batch_size in range(0, len(questions), 5):\n",
    "        batch_qs = questions[batch_size:batch_size+5]\n",
    "\n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
    " \n",
    "        for query, response in zip(batch_qs, responses):\n",
    "    \n",
    "            if evaluator.evaluate_response(query=query, response=response).passing: \n",
    "                eval_result = 1\n",
    "            else:\n",
    "                eval_result = 0\n",
    "            total_correct += eval_result\n",
    "            all_results.append(eval_result)\n",
    "        \n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished batch 1 out of 8\n",
      "finished batch 2 out of 8\n",
      "finished batch 3 out of 8\n",
      "finished batch 4 out of 8\n",
      "finished batch 5 out of 8\n",
      "finished batch 6 out of 8\n",
      "finished batch 7 out of 8\n",
      "finished batch 8 out of 8\n",
      "Response satisfies the query? Scored 20 out of 40 questions correctly.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import QueryResponseEvaluator\n",
    "\n",
    "evaluator = QueryResponseEvaluator(service_context=gpt4_service_context)\n",
    "\n",
    "#evaluator = RelevancyEvaluator(service_context=service_context)\n",
    "\n",
    "# query index\n",
    "#query_engine = vector_index.as_query_engine()\n",
    "#query = \"What battles took place in New York City in the American Revolution?\"\n",
    "#response = query_engine.query(query)\n",
    "#eval_result = evaluator.evaluate_response(query=query, response=response)\n",
    "#print(str(eval_result))\n",
    "\n",
    "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
    "\n",
    "print(f\"Response satisfies the query? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Incorrect Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How can I convert tools to LangChain tools using the provided documentation sample?'\n",
      " 'What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SubQuestionQueryEngine class in LlamaIndex?'\n",
      " 'What is the purpose of the `query_wrapper_prompt` in the `HuggingFaceLLM` class?'\n",
      " 'What are the available options for the storage backend of the index store in LlamaIndex?'\n",
      " 'What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the DEFAULT_REFINE_PROMPT_SEL_LC in the LlamaIndex documentation?'\n",
      " \"What is the purpose of the `CollectionQueryConsumer` class in the Delphic application's WebSocket handling?\"\n",
      " 'How can I create a Django superuser using the Delphic application?'\n",
      " 'What is the purpose of the \"router query engine\" in the LlamaIndex framework?'\n",
      " 'What is the purpose of the `VectorStoreIndex` class in the LlamaIndex documentation sample?'\n",
      " 'What is the purpose of the `RefinePrompt` class in the LlamaIndex documentation?'\n",
      " \"What is the function used to retrieve the collections for the logged-in user in the Delphic project's frontend?\"\n",
      " 'What is the purpose of the TokenCountingHandler callback in the LlamaIndex library?'\n",
      " 'What is the purpose of the ResponseEvaluator class in the LlamaIndex library?'\n",
      " 'What is the purpose of the Algovera tool built on top of LlamaIndex?'\n",
      " 'What are the three primary sections within the layout of the ChatView component?'\n",
      " 'What is the default value for the child_branch_factor parameter when configuring the ComposableGraphQueryEngine?'\n",
      " 'What is the purpose of the `load_collection_model` function in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SQLTableNodeMapping object in the LlamaIndex documentation sample?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unanswered_queries = np.array(question_dataset)[np.array(all_results) == 0]\n",
    "print(unanswered_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `ReActAgent` is not provided in the given context information. However, the `ReActAgent` can be initialized with other agents as tools by creating instances of the desired agents and passing them as arguments to the `QueryEngineTool` constructor. These instances are then added to a list of query engine tools, along with their corresponding metadata. Finally, the `ReActAgent` is instantiated using the `from_tools()` method, which takes the list of query engine tools as an argument.\n",
      "-----------------\n",
      "> Source (Doc id: e745971e-773c-4e73-a6e5-802ddc1dbe64): Sub question: What is the purpose of the ReActAgent?\n",
      "Response: The purpose of the ReActAgent is not provided in the given context information.\n",
      "\n",
      "> Source (Doc id: f860a27d-5f2f-4748-9164-c96e7534d762): Sub question: How can the ReActAgent be initialized with other agents as tools?\n",
      "Response: The ReActAgent can be initialized with other agents as tools by creating instances of the desired agents and passing them as arguments to the QueryEngineTool const...\n",
      "\n",
      "> Source (Doc id: 88c22414-a322-4955-9b4f-2b46254c11ea): An agent is initialized from a set of Tools. Here's an example of instantiating a ReAct\n",
      "agent from a set of Tools.\n",
      "\n",
      "> Source (Doc id: 22cc3aeb-7258-445b-a526-6f636b868aef): ```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/examples/agent/react_agent_with_query_engine.ipynb\n",
      "```\n",
      "\n",
      "> Source (Doc id: 88c22414-a322-4955-9b4f-2b46254c11ea): An agent is initialized from a set of Tools. Here's an example of instantiating a ReAct\n",
      "agent from a set of Tools.\n",
      "\n",
      "> Source (Doc id: 1b4b006f-68c7-44f6-ac6d-98e1b54911c2): from llama_index.tools import QueryEngineTool\n",
      "\n",
      "query_engine_tools = [\n",
      "    QueryEngineTool(\n",
      "        query_engine=sql_agent,\n",
      "        metadata=ToolMetadata(\n",
      "            name=\"sql_agent\",\n",
      "            description=\"Agent that can execute SQL queries.\"\n",
      "       ...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the purpose of the `ReActAgent` and how can it be initialized with other agents as tools?')\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is not mentioned in the given context information.\n",
      "-----------------\n",
      "> Source (Doc id: 2fdce762-1dd8-4454-94b8-0342e1f6e527): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is not mentioned in the given context information.\n",
      "\n",
      "> Source (Doc id: 3f72f10f-76f1-462c-b4b0-cc59fda677c7): Sub question: How does the LoadAndSearchToolSpec work in the LlamaIndex documentation?\n",
      "Response: The LoadAndSearchToolSpec is not mentioned in the given context information.\n",
      "\n",
      "> Source (Doc id: 71a15480-fa4a-4603-8729-f60b77736e1d): Sub question: What are the features of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The features of the LoadAndSearchToolSpec in the LlamaIndex documentation are not mentioned in the given context information.\n",
      "\n",
      "> Source (Doc id: a048c2cb-f494-43b5-bfaa-d97e4a934ba8): LlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.\n",
      "\n",
      "Under the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:\n",
      "\n",
      "- **Document stores**: where ingested document...\n",
      "\n",
      "> Source (Doc id: d94388e8-77be-4c30-834d-b8f9d1a9351d): During query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\n",
      "our Response Synthesis module.\n",
      "\n",
      "!\n",
      "\n",
      "The list index does offer numerous ways of querying a list index, from an embedding-based query which \n",
      "w...\n",
      "\n",
      "> Source (Doc id: d94388e8-77be-4c30-834d-b8f9d1a9351d): During query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\n",
      "our Response Synthesis module.\n",
      "\n",
      "!\n",
      "\n",
      "The list index does offer numerous ways of querying a list index, from an embedding-based query which \n",
      "w...\n",
      "\n",
      "> Source (Doc id: 586b1159-e073-4b30-ac24-7f791338507f): This guide describes how each index works with diagrams. \n",
      "\n",
      "Some terminology:\n",
      "- **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\n",
      "- **Response Synthesis**: ...\n",
      "\n",
      "> Source (Doc id: a048c2cb-f494-43b5-bfaa-d97e4a934ba8): LlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.\n",
      "\n",
      "Under the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:\n",
      "\n",
      "- **Document stores**: where ingested document...\n",
      "\n",
      "> Source (Doc id: d94388e8-77be-4c30-834d-b8f9d1a9351d): During query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\n",
      "our Response Synthesis module.\n",
      "\n",
      "!\n",
      "\n",
      "The list index does offer numerous ways of querying a list index, from an embedding-based query which \n",
      "w...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?')\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=256))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we covered several key topics!\n",
    "\n",
    "- setting up a sub-question query engine\n",
    "- generating a dataset of evaluation questions\n",
    "- evaluating responses for hallucination\n",
    "- evaluating responses for answer quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
